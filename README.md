K-NearestNeighbor
=======================================================================

## **1.最近邻**
Nearest Neighbors官方网址：
https://scikit-learn.org/stable/modules/neighbors.html

GitHub文档地址：https://github.com/gao7025/iris_knn

## **2.工作原理**
KNN是通过测量不同特征值之间的距离进行分类。它的思路是：K个最近的邻居，每个样本都可以用它最接近的K个邻居来代表，如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特征， KNN算法的结果很大程度取决于K的选择，其中K通常是不大于20的整数。距离有很多种计算方法，一般使用欧氏距离或曼哈顿距离即可。
                      
换句话说，KNN就是在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，当K取值较大时，抗噪能力强，但K取值较小时，对噪声点的影响特别敏感。

**主要步骤**：

 1. 计算测试数据与各个训练数据之间的距离；
 2. 按照距离的递增关系进行排序；
 3. 选取距离最小的K个点；
 4. 确定前K个点所在类别的出现频率；
 5. 返回前K个点中出现频率最高的类别作为测试数据的预测分类。


 - 优点：
  （1）简单方便，易于理解，无需训练，直接计算距离  
  （2）适用于多分类问题  
 - 缺点：
 （1）当样本数据不平衡时，分类的效果不佳  
 （2）计算量较大，每一点都要计算K个最近的邻居的距离
